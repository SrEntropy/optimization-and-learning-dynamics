# Experiments
- 1 XOR From Scratch
  - Show failure with linear model
  - Show success with MLP
  - Explain representational necesity

- 2 Vanishing/Exploding Gradients
  - Deep tanh network
  - Show gradient norm across layers
- 3 Stability of Training
  - Large vs small step size
  - Oscillation vs convergence
- 4 Gradient Flow vs GD
  - Same loss, different dynamics
  - insightful plots