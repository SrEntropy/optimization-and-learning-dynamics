## Stage 1:
```text
| Experiment | Demonstrates                             |
| ---------- | ---------------------------------------- |
| 1          | Local derivative shapes learning signal  |
| 2          | Depth causes gradient decay              |
| 3          | Population encoding increases robustness |
| 4          | Proper credit assignment                 |
| 5          | Reduction compresses information         |
| 6          | Symmetry breaking without noise          |
| 7          | Learning signals exist without learning  |
```
## Stage 2:
```text
| Experiment | Demonstrates                             |
| ---------- | ---------------------------------------- |
| 1          | Learning fails while gradient are correct|
| 2          | Step size as stability regim in learning |
| 3          | GD as discrete-time dynamical system     |
| 4          | Symmetry: Equivariant under permutation  |
| 5          | Assymetry: Gradient differs, noise, etc. |
| 6          | Parameters evolve and nodes are inter-sig|
| 7          | Learning signals exist without learning  |
| 8          | Instability in Math: perturbation grows_t|
| 7          | Failure reveals limitations of systems   |
```

