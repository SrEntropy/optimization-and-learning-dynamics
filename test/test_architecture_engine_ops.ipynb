{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6896968b-1807-4177-89a8-c429b4b3c554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"src\")\n",
    "\n",
    "from core.populationNode import PopulationNode\n",
    "from core.ops import _as_node, _broadcast_to_match, add, sub, mul, sum_pop, matvec\n",
    "from core.parameter import Parameter\n",
    "from models.activations import tanh\n",
    "from core.optim import GD, Momentum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9881a868-caaf-48ad-add6-c418a975dfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NODE] op=tanh, value=[0.4134608463957053], grad=[1.0] | <-- Parents=[[0.43977852653283767]]\n",
      "[NODE] op=tanh, value=[0.43977852653283767], grad=[0.829050128497747] | <-- Parents=[[0.4719561929220392]]\n",
      "[NODE] op=tanh, value=[0.4719561929220392], grad=[0.6687075620489031] | <-- Parents=[[0.5125841451937511]]\n",
      "[NODE] op=tanh, value=[0.5125841451937511], grad=[0.519757868915453] | <-- Parents=[[0.5662285757646344]]\n",
      "[NODE] op=tanh, value=[0.5662285757646344], grad=[0.3831953839732797] | <-- Parents=[[0.6419540521719098]]\n",
      "[NODE] op=tanh, value=[0.6419540521719098], grad=[0.26033727257499406] | <-- Parents=[[0.7614904913621627]]\n",
      "[NODE] op=tanh, value=[0.7614904913621627], grad=[0.15305097953277294] | <-- Parents=[[0.9997532108480275]]\n",
      "[NODE] op=tanh, value=[0.9997532108480275], grad=[0.06430164957431488] | <-- Parents=[[4.5]]\n",
      "[NODE] op=leaf, value=[4.5], grad=[3.1733982853148134e-05] | <-- Parents=[]\n"
     ]
    }
   ],
   "source": [
    "x = PopulationNode([4.5])\n",
    "y1 = x.tanh()\n",
    "y2 = y1.tanh()\n",
    "y3 = y2.tanh()\n",
    "y4 = y3.tanh()\n",
    "y5 = y4.tanh()\n",
    "y6 = y5.tanh()\n",
    "y7 = y6.tanh()\n",
    "y8 = y7.tanh()\n",
    "y8.backprop(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74335c2f-e8e1-4b7e-8892-5ad3a8850ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  PopulationNode( data=[-6.0, -3.0, -0.5, 0.0, 0.5, 3.0, 6.0], grad=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], op='leaf')\n",
      "[NODE] op=sum, value=[0.0], grad=[1.0] | <-- Parents=[[-0.9999877116507956, -0.9950547536867305, -0.46211715726000974, 0.0, 0.46211715726000974, 0.9950547536867305, 0.9999877116507956]]\n",
      "[NODE] op=tanh, value=[-0.9999877116507956, -0.9950547536867305, -0.46211715726000974, 0.0, 0.46211715726000974, 0.9950547536867305, 0.9999877116507956], grad=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0] | <-- Parents=[[-6.0, -3.0, -0.5, 0.0, 0.5, 3.0, 6.0]]\n",
      "[NODE] op=leaf, value=[-6.0, -3.0, -0.5, 0.0, 0.5, 3.0, 6.0], grad=[2.4576547405286142e-05, 0.009866037165440211, 0.7864477329659274, 1.0, 0.7864477329659274, 0.009866037165440211, 2.4576547405286142e-05] | <-- Parents=[]\n"
     ]
    }
   ],
   "source": [
    "xs = [-6, -3.0, -0.5, 0, .5, 3.0, 6]\n",
    "x = PopulationNode(xs)\n",
    "print(\"x: \", x)\n",
    "y = x.tanh()\n",
    "\n",
    "sum_pop(y).backprop(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f5e91af-f291-480c-a63c-0a3fdb42da53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum of unit gradient 0.7864477329659274\n"
     ]
    }
   ],
   "source": [
    "x_scalar = PopulationNode(0.5)\n",
    "y_scalar = x_scalar.tanh()\n",
    "y_scalar.backprop()\n",
    "print(\"sum of unit gradient\", sum(x_scalar.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13754c51-e757-4b6c-ac78-a9ad08a710b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3 [0.009866037165440211]\n",
      "-1 [0.41997434161402614]\n",
      "0 [1.0]\n",
      "1 [0.41997434161402614]\n",
      "3 [0.009866037165440211]\n"
     ]
    }
   ],
   "source": [
    "for v in [-3, -1, 0, 1, 3]:\n",
    "    x = PopulationNode(v)\n",
    "    y = x.tanh()\n",
    "    y.backprop()\n",
    "    print(v, x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c2c4504-f0fa-450a-993f-5b0f0dea8efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4.4, 4.4, 4.4],\n",
       " [3.9200000000000004, 3.9200000000000004, 3.9200000000000004],\n",
       " [3.5360000000000005, 3.5360000000000005, 3.5360000000000005],\n",
       " [3.2288000000000006, 3.2288000000000006, 3.2288000000000006],\n",
       " [2.9830400000000004, 2.9830400000000004, 2.9830400000000004],\n",
       " [2.7864320000000005, 2.7864320000000005, 2.7864320000000005],\n",
       " [2.6291456, 2.6291456, 2.6291456],\n",
       " [2.50331648, 2.50331648, 2.50331648],\n",
       " [2.402653184, 2.402653184, 2.402653184],\n",
       " [2.3221225472, 2.3221225472, 2.3221225472],\n",
       " [2.25769803776, 2.25769803776, 2.25769803776],\n",
       " [2.206158430208, 2.206158430208, 2.206158430208],\n",
       " [2.1649267441664, 2.1649267441664, 2.1649267441664],\n",
       " [2.13194139533312, 2.13194139533312, 2.13194139533312],\n",
       " [2.105553116266496, 2.105553116266496, 2.105553116266496]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "w = Parameter([5.0, 5.0, 5.0])\n",
    "opt = GD([w], lr=0.1)\n",
    "\n",
    "def pop_loss(w):\n",
    "    return sum((wi - 2.0) ** 2 for wi in w.data)\n",
    "\n",
    "history = []\n",
    "\n",
    "for _ in range(15):\n",
    "    L = pop_loss(w)\n",
    "    for i in range(len(w.data)):\n",
    "        w.grad[i] = 2 * (w.data[i] - 2.0)\n",
    "    opt.step()\n",
    "    history.append(w.data.copy())\n",
    "    opt.zero_grad()\n",
    "\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7d5e210-c552-4254-b9f6-c29821850d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4.4, 4.4799999999999995, 4.32],\n",
       " [3.9200000000000004, 3.9839999999999995, 3.8560000000000003],\n",
       " [3.5360000000000005, 3.5871999999999997, 3.4848000000000003],\n",
       " [3.2288000000000006, 3.2697599999999998, 3.1878400000000005],\n",
       " [2.9830400000000004, 3.015808, 2.9502720000000004],\n",
       " [2.7864320000000005, 2.8126463999999998, 2.7602176000000003],\n",
       " [2.6291456, 2.65011712, 2.6081740800000004],\n",
       " [2.50331648, 2.520093696, 2.486539264],\n",
       " [2.402653184, 2.4160749568, 2.3892314112],\n",
       " [2.3221225472, 2.33285996544, 2.31138512896],\n",
       " [2.25769803776, 2.266287972352, 2.249108103168],\n",
       " [2.206158430208, 2.2130303778816, 2.1992864825344],\n",
       " [2.1649267441664, 2.17042430230528, 2.15942918602752],\n",
       " [2.13194139533312, 2.136339441844224, 2.127543348822016],\n",
       " [2.105553116266496, 2.109071553475379, 2.1020346790576125]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = Parameter([5.0, 5.1, 4.9])\n",
    "opt = GD([w], lr=0.1)\n",
    "\n",
    "history_asym = []\n",
    "\n",
    "for _ in range(15):\n",
    "    for i in range(len(w.data)):\n",
    "        w.grad[i] = 2 * (w.data[i] - 2.0)\n",
    "    opt.step()\n",
    "    history_asym.append(w.data.copy())\n",
    "    opt.zero_grad()\n",
    "\n",
    "history_asym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e20e634-8012-41da-9762-5d7e165aefb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def _almost_equal_list(a, b, tol=1e-6):\n",
    "    assert len(a) == len(b)\n",
    "    for x, y in zip(a, b):\n",
    "        assert abs(x - y) < tol\n",
    "\n",
    "\n",
    "def test_matvec_forward_backward_diagonal():\n",
    "    A = [[2.0, 0.0],\n",
    "         [0.0, 3.0]]\n",
    "\n",
    "    x = PopulationNode([1.0, 1.0], requires_grad=True)\n",
    "    y = matvec(A, x)          # [2,3]\n",
    "    loss = sum_pop(y)         # 5\n",
    "\n",
    "    loss.zero_grad_graph()\n",
    "    loss.backprop(debug=True)\n",
    "\n",
    "    assert y.data == [2.0, 3.0]\n",
    "    # dL/dx = A^T @ [1,1] = [2,3]\n",
    "    _almost_equal_list(x.grad, [2.0, 3.0])\n",
    "\n",
    "\n",
    "def test_matvec_forward_backward_random():\n",
    "    rng = np.random.default_rng(0)\n",
    "    A = rng.normal(size=(3, 2))\n",
    "    x0 = rng.normal(size=(2,))\n",
    "\n",
    "    x = PopulationNode(x0.tolist(), requires_grad=True)\n",
    "    y = matvec(A, x)\n",
    "    loss = sum_pop(y)\n",
    "\n",
    "    loss.zero_grad_graph()\n",
    "    loss.backprop()\n",
    "\n",
    "    # analytic: A^T @ ones(3)\n",
    "    expected = A.T @ np.ones(3)\n",
    "    got = np.array(x.grad)\n",
    "    assert np.max(np.abs(got - expected)) < 1e-6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b96d2710-9403-4c2f-9c9c-11a9e01134e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NODE] op=sum, value=[5.0], grad=[1.0] | <-- Parents=[[2.0, 3.0]]\n",
      "[NODE] op=matvec, value=[2.0, 3.0], grad=[1.0, 1.0] | <-- Parents=[[1.0, 1.0]]\n",
      "[NODE] op=leaf, value=[1.0, 1.0], grad=[2.0, 3.0] | <-- Parents=[]\n"
     ]
    }
   ],
   "source": [
    "test_matvec_forward_backward_diagonal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2111678-733d-4360-94f1-82bfc2d0e6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_matvec_forward_backward_random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a534792c-43d4-426c-89f5-dadad95ece35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Momentum tests passed.\n"
     ]
    }
   ],
   "source": [
    "def close(a, b, tol=1e-9):\n",
    "    return all(abs(x - y) < tol for x, y in zip(a, b))\n",
    "\n",
    "def test_momentum_beta_zero_equals_gd_update():\n",
    "    \"\"\"\n",
    "    If beta = 0, then:\n",
    "        v <- -lr * grad\n",
    "        w <- w + v = w - lr * grad\n",
    "    So it should behave like plain GD for that step.\n",
    "    \"\"\"\n",
    "    w = Parameter([1.0, 2.0])\n",
    "    w.grad = [1.0, 1.0]\n",
    "\n",
    "    opt = Momentum([w], lr=0.1, beta=0.0)\n",
    "    opt.step()\n",
    "\n",
    "    assert close(w.data, [0.9, 1.9])\n",
    "\n",
    "\n",
    "def test_momentum_two_steps_matches_hand_computation():\n",
    "    \"\"\"\n",
    "    Hand-check 2 steps with constant gradient to verify velocity accumulation.\n",
    "\n",
    "    Let w0 = 1.0, grad = 1.0, lr = 0.1, beta = 0.9\n",
    "    v0 = 0\n",
    "\n",
    "    Step 1:\n",
    "      v1 = 0.9*v0 - 0.1*1 = -0.1\n",
    "      w1 = w0 + v1 = 0.9\n",
    "\n",
    "    Step 2:\n",
    "      v2 = 0.9*v1 - 0.1*1 = 0.9*(-0.1) - 0.1 = -0.19\n",
    "      w2 = w1 + v2 = 0.9 - 0.19 = 0.71\n",
    "    \"\"\"\n",
    "    w = Parameter([1.0])\n",
    "    opt = Momentum([w], lr=0.1, beta=0.9)\n",
    "\n",
    "    # Step 1\n",
    "    w.grad = [1.0]\n",
    "    opt.step()\n",
    "    assert close(w.data, [0.9])\n",
    "\n",
    "    # Step 2 (same grad)\n",
    "    w.grad = [1.0]\n",
    "    opt.step()\n",
    "    assert close(w.data, [0.71])\n",
    "\n",
    "\n",
    "def test_momentum_zero_grad_clears_param_grads_only():\n",
    "    \"\"\"\n",
    "    zero_grad should clear Parameter.grad.\n",
    "    It should NOT reset velocity unless you explicitly add such behavior.\n",
    "    \"\"\"\n",
    "    w = Parameter([1.0])\n",
    "    opt = Momentum([w], lr=0.1, beta=0.9)\n",
    "\n",
    "    w.grad = [1.0]\n",
    "    opt.step()  # now velocity is non-zero\n",
    "\n",
    "    w.grad = [5.0]\n",
    "    opt.zero_grad()\n",
    "    assert w.grad == [0.0]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run without pytest\n",
    "    test_momentum_beta_zero_equals_gd_update()\n",
    "    test_momentum_two_steps_matches_hand_computation()\n",
    "    test_momentum_zero_grad_clears_param_grads_only()\n",
    "    print(\"All Momentum tests passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94a61ced-4c53-4d83-a27b-e0e44c5075aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Test 1: Scalar Test (Simple Chain Rule)\n",
      "============================================================\n",
      "z =  (expected [8.0]): [8.0]\n",
      "x.grad (expected [4.0]): [4.0]\n",
      "y.grad (expected [2.0]): [2.0]\n",
      "✓ Passed: gradients match analytical derivatives\n",
      "\n",
      "============================================================\n",
      "Test 2: Vector Test (Vector Chain Rule)\n",
      "============================================================\n",
      "x.grad (expected [1.0, 1.0]): [1.0, 1.0]\n",
      "y.grad (expected [1.0, 1.0]): [1.0, 1.0]\n",
      "✓ Passed: vector gradients match analytical derivatives\n",
      "\n",
      "============================================================\n",
      "Test 3: (w * x) -> sum -> + b -> tanh\n",
      "============================================================\n",
      "a.data: [0.7615941559557649]\n",
      "w.grad: [0.41997434161402614, 0.0] (expected [0.41997434161402614, 0.0])\n",
      "x.grad: [0.41997434161402614, -0.41997434161402614] (expected [0.41997434161402614, -0.41997434161402614])\n",
      "b.grad: [0.41997434161402614] (expected [0.41997434161402614])\n",
      "✓ Passed: tanh derivative and neuron gradients are correct\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def header(title):\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(title)\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "def close(a, b, tol=1e-9):\n",
    "    return all(abs(x - y) < tol for x, y in zip(a, b))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Test 1: Scalar test\n",
    "# ------------------------------------------------------------\n",
    "header(\"Test 1: Scalar Test (Simple Chain Rule)\")\n",
    "\n",
    "x = PopulationNode(2.0)\n",
    "y = PopulationNode(3.0)\n",
    "z = x * y + x\n",
    "print(f\"z =  (expected [8.0]): {z.data}\")\n",
    "z.backprop()\n",
    "\n",
    "print(f\"x.grad (expected [4.0]): {x.grad}\")\n",
    "print(f\"y.grad (expected [2.0]): {y.grad}\")\n",
    "\n",
    "assert close(x.grad, [4.0]), \"Incorrect gradient for x\"\n",
    "assert close(y.grad, [2.0]), \"Incorrect gradient for y\"\n",
    "\n",
    "print(\"✓ Passed: gradients match analytical derivatives\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Test 2: Vector test\n",
    "# ------------------------------------------------------------\n",
    "header(\"Test 2: Vector Test (Vector Chain Rule)\")\n",
    "\n",
    "x = PopulationNode([1.0, 2.0])\n",
    "y = PopulationNode([3.0, 4.0])\n",
    "z = sum_pop(x + y)\n",
    "z.backprop()\n",
    "\n",
    "print(f\"x.grad (expected [1.0, 1.0]): {x.grad}\")\n",
    "print(f\"y.grad (expected [1.0, 1.0]): {y.grad}\")\n",
    "\n",
    "assert close(x.grad, [1.0, 1.0]), \"Incorrect gradient for x\"\n",
    "assert close(y.grad, [1.0, 1.0]), \"Incorrect gradient for y\"\n",
    "\n",
    "print(\"✓ Passed: vector gradients match analytical derivatives\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Test 3: Elementwise multiply + sum reduction + tanh neuron\n",
    "# ------------------------------------------------------------\n",
    "header(\"Test 3: (w * x) -> sum -> + b -> tanh\")\n",
    "\n",
    "w = PopulationNode([1.0, -1.0])\n",
    "x = PopulationNode([1.0, 0.0])\n",
    "b = PopulationNode(0.0)\n",
    "\n",
    "z = sum_pop(w * x) + b\n",
    "a = z.tanh()\n",
    "a.backprop()\n",
    "\n",
    "dadz = 1.0 - a.data[0] ** 2\n",
    "expected_w = [dadz * x.data[0], dadz * x.data[1]]\n",
    "expected_x = [dadz * w.data[0], dadz * w.data[1]]\n",
    "expected_b = [dadz]\n",
    "\n",
    "print(f\"a.data: {a.data}\")\n",
    "print(f\"w.grad: {w.grad} (expected {expected_w})\")\n",
    "print(f\"x.grad: {x.grad} (expected {expected_x})\")\n",
    "print(f\"b.grad: {b.grad} (expected {expected_b})\")\n",
    "\n",
    "assert close(w.grad, expected_w), \"Incorrect gradient for weights\"\n",
    "assert close(x.grad, expected_x), \"Incorrect gradient for inputs\"\n",
    "assert close(b.grad, expected_b), \"Incorrect gradient for bias\"\n",
    "\n",
    "print(\"✓ Passed: tanh derivative and neuron gradients are correct\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48174ac6-b33e-455a-b3c2-867cb528ba6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Test 4: Gradient accumulation (no zero_grad)\n",
      "============================================================\n",
      "Z:  PopulationNode( data=[8.0], grad=[0.0], op='+')\n",
      "-----------Before backprop-------------\n",
      "pre-GZ:  [0.0]\n",
      "pre-Gy:  [0.0]\n",
      "pre-Gx:  [0.0]\n",
      "-----------After second backprop-------------\n",
      "Z:  PopulationNode( data=[8.0], grad=[1.0], op='+')\n",
      "pre-GZ:  [1.0]\n",
      "pre-Gy:  [2.0]\n",
      "pre-Gx:  [4.0]\n",
      "pre-g1x:  [4.0] pre-g2y:  [2.0]\n",
      "-----------Before second backprop-------------\n",
      "pre-GZ:  [1.0]\n",
      "pre-Gy:  [2.0]\n",
      "pre-Gx:  [4.0]\n",
      "-----------After second backprop-------------\n",
      "pre-GZ:  [1.0]\n",
      "pre-Gy:  [4.0]\n",
      "pre-Gx:  [8.0]\n",
      "pre-g1x:  [8.0] pre-g2y:  [4.0]\n",
      "\n",
      " ----Checking------\n",
      "g2x:  [8.0] g2y:  [4.0]\n",
      "First x.grad: [4.0], second x.grad: [8.0]\n",
      "First y.grad: [2.0], second y.grad: [4.0]\n",
      "✓ Passed: gradients accumulate as expected\n",
      "\n",
      "All tests passed successfully.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Test 4: Gradient accumulation\n",
    "# ------------------------------------------------------------\n",
    "header(\"Test 4: Gradient accumulation (no zero_grad)\")\n",
    "\n",
    "x = PopulationNode(2.0)\n",
    "y = PopulationNode(3.0)\n",
    "z = x * y + x\n",
    "print(\"Z: \",z)\n",
    "print(\"-----------Before backprop-------------\")\n",
    "print(\"pre-GZ: \",z.grad)\n",
    "print(\"pre-Gy: \",y.grad)\n",
    "print(\"pre-Gx: \",x.grad)\n",
    "z.backprop()\n",
    "print(\"-----------After second backprop-------------\")\n",
    "print(\"Z: \",z)\n",
    "print(\"pre-GZ: \",z.grad)\n",
    "print(\"pre-Gy: \",y.grad)\n",
    "print(\"pre-Gx: \",x.grad)\n",
    "\n",
    "g1x, g1y = x.grad[:], y.grad[:]\n",
    "print(\"pre-g1x: \", g1x, \"pre-g2y: \", g1y)\n",
    "print(\"-----------Before second backprop-------------\")\n",
    "print(\"pre-GZ: \",z.grad)\n",
    "print(\"pre-Gy: \",y.grad)\n",
    "print(\"pre-Gx: \",x.grad)\n",
    "z.backprop()  # no reset\n",
    "print(\"-----------After second backprop-------------\")\n",
    "\n",
    "print(\"pre-GZ: \",z.grad)\n",
    "print(\"pre-Gy: \",y.grad)\n",
    "print(\"pre-Gx: \",x.grad)\n",
    "g2x, g2y = x.grad[:], y.grad[:]\n",
    "print(\"pre-g1x: \", g2x, \"pre-g2y: \", g2y)\n",
    "print(\"\\n ----Checking------\")\n",
    "print(\"g2x: \", g2x, \"g2y: \", g2y)\n",
    "print(f\"First x.grad: {g1x}, second x.grad: {g2x}\")\n",
    "print(f\"First y.grad: {g1y}, second y.grad: {g2y}\")\n",
    "\n",
    "assert close(g2x, [2 * g1x[0]]), \"Gradients should accumulate for x\"\n",
    "assert close(g2y, [2 * g1y[0]]), \"Gradients should accumulate for y\"\n",
    "print(\"✓ Passed: gradients accumulate as expected\")\n",
    "\n",
    "print(\"\\nAll tests passed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f01685d-c1e1-42e4-ac3b-19f510ffbe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "w1 = Parameter([1.0])\n",
    "w2 = Parameter([2.0])\n",
    "\n",
    "w1.grad = [1.0]\n",
    "w2.grad = [2.0]\n",
    "\n",
    "opt = GD([w1, w2], lr=0.1)\n",
    "opt.step()\n",
    "\n",
    "assert w1.data == [0.9]\n",
    "assert w2.data == [1.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1bd408c-d907-4ff1-bc4a-28e9d6dc8771",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.zero_grad()\n",
    "assert w1.grad == [0.0]\n",
    "assert w2.grad == [0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c42b23c-d8bf-4ada-92d5-6d2ebd9f6d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test_grad_accumulates_then_resets():\n",
    "    x = PopulationNode([1.0, 2.0], requires_grad=True)\n",
    "    print(x)\n",
    "    loss = sum_pop(x)  # loss = x1 + x2\n",
    "    print(x)\n",
    "    print(\"loss: \",loss)\n",
    "    loss.zero_grad_graph()\n",
    "    print(\"loss: \",loss)\n",
    "    loss.backprop(debug=True)\n",
    "    print(\"loss: \",loss)\n",
    "    g1 = list(x.grad)\n",
    "    print(g1)\n",
    "    assert g1 == [1.0, 1.0]\n",
    "    print(\"✓ Passed: gradients reset as expected\")\n",
    "\n",
    "    # backprop again without clearing => accumulates\n",
    "    loss.backprop(debug=True)\n",
    "    g2 = list(x.grad)\n",
    "    assert g2 == [2.0, 2.0]\n",
    "    print(\"✓ Passed: gradients accumulate as expected\")\n",
    "\n",
    "    # clear entire graph\n",
    "    loss.zero_grad_graph()\n",
    "    assert x.grad == [0.0, 0.0]\n",
    "    print(\"✓ Passed: clear entire graph's gradient as expected\")\n",
    "\n",
    "    print(\"\\nAll tests passed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "909f916b-04c3-462e-a1c8-a7f86a6212d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PopulationNode( data=[1.0, 2.0], grad=[0.0, 0.0], op='leaf')\n",
      "PopulationNode( data=[1.0, 2.0], grad=[0.0, 0.0], op='leaf')\n",
      "loss:  PopulationNode( data=[3.0], grad=[0.0], op='sum')\n",
      "loss:  PopulationNode( data=[3.0], grad=[0.0], op='sum')\n",
      "[NODE] op=sum, value=[3.0], grad=[1.0] | <-- Parents=[[1.0, 2.0]]\n",
      "[NODE] op=leaf, value=[1.0, 2.0], grad=[1.0, 1.0] | <-- Parents=[]\n",
      "loss:  PopulationNode( data=[3.0], grad=[1.0], op='sum')\n",
      "[1.0, 1.0]\n",
      "✓ Passed: gradients reset as expected\n",
      "[NODE] op=sum, value=[3.0], grad=[1.0] | <-- Parents=[[1.0, 2.0]]\n",
      "[NODE] op=leaf, value=[1.0, 2.0], grad=[2.0, 2.0] | <-- Parents=[]\n",
      "✓ Passed: gradients accumulate as expected\n",
      "✓ Passed: clear entire graph's gradient as expected\n",
      "\n",
      "All tests passed successfully.\n"
     ]
    }
   ],
   "source": [
    "test_grad_accumulates_then_resets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd0b722-74e1-42a1-a653-5c17be2bbc58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ce60577-5a64-4a87-b134-485e3429c8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_population_autodiff_engine():\n",
    "    # ------------------------------------------------------------\n",
    "    # 1. Leaf creation & shape\n",
    "    # ------------------------------------------------------------\n",
    "    x = PopulationNode([1.0, 2.0], requires_grad=True)\n",
    "    y = PopulationNode([3.0, 4.0], requires_grad=True)\n",
    "    assert x.data == [1.0, 2.0]\n",
    "    assert y.data == [3.0, 4.0]\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 2. Elementwise ops: add, sub, mul\n",
    "    # ------------------------------------------------------------\n",
    "    z1 = x + y\n",
    "    assert z1.data == [4.0, 6.0]\n",
    "\n",
    "    z2 = x - y\n",
    "    assert z2.data == [-2.0, -2.0]\n",
    "\n",
    "    z3 = x * y\n",
    "    assert z3.data == [3.0, 8.0]\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 3. Nonlinearity: tanh\n",
    "    # ------------------------------------------------------------\n",
    "    t = x.tanh()\n",
    "    import math\n",
    "    assert t.data == [math.tanh(1.0), math.tanh(2.0)]\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 4. Reduction: sum\n",
    "    # ------------------------------------------------------------\n",
    "    s = sum_pop(x)\n",
    "    assert s.data == [3.0]\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 5. Backprop through simple graph\n",
    "    #    s = x.sum() => ds/dx = [1, 1]\n",
    "    # ------------------------------------------------------------\n",
    "    x.zero_grad()\n",
    "    s.backprop()\n",
    "    assert x.grad == [1.0, 1.0]\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 6. Multi-path graph:\n",
    "    #    z = x*y + x\n",
    "    #    dz/dx = y + 1\n",
    "    #    dz/dy = x\n",
    "    # ------------------------------------------------------------\n",
    "    x = PopulationNode([2.0], requires_grad=True)\n",
    "    y = PopulationNode([3.0], requires_grad=True)\n",
    "    z = x * y + x\n",
    "\n",
    "    x.zero_grad()\n",
    "    y.zero_grad()\n",
    "    z.backprop()\n",
    "\n",
    "    assert x.grad == [4.0]   # y + 1 = 3 + 1\n",
    "    assert y.grad == [2.0]   # x = 2\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 7. Gradient accumulation on leaf nodes\n",
    "    # ------------------------------------------------------------\n",
    "    z.backprop()  # second backward, no zero_grad\n",
    "\n",
    "    assert x.grad == [8.0]   # 4 + 4\n",
    "    assert y.grad == [4.0]   # 2 + 2\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 8. Intermediate nodes must NOT accumulate\n",
    "    #    (We check this indirectly by verifying correct doubling)\n",
    "    # ------------------------------------------------------------\n",
    "    # If intermediate nodes accumulated, x.grad would be wrong (11, 12, etc.)\n",
    "    assert x.grad == [8.0]\n",
    "    assert y.grad == [4.0]\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 9. zero_grad_graph clears entire graph\n",
    "    # ------------------------------------------------------------\n",
    "    z.zero_grad_graph()\n",
    "    assert x.grad == [0.0]\n",
    "    assert y.grad == [0.0]\n",
    "    # intermediate nodes should also be zeroed (implicitly tested)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 10. Vector-output backward\n",
    "    # ------------------------------------------------------------\n",
    "    v = PopulationNode([1.0, 2.0, 3.0], requires_grad=True)\n",
    "    out = v * 2.0  # elementwise\n",
    "    v.zero_grad()\n",
    "    out.backprop(seed_grad=[1.0, 1.0, 1.0])\n",
    "    assert v.grad == [2.0, 2.0, 2.0]\n",
    "    print(\"✓ Full engine test passed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18a3a367-f480-442e-94e9-92a5d6992f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Full engine test passed successfully.\n"
     ]
    }
   ],
   "source": [
    "test_population_autodiff_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb632e5a-0cb3-48ae-acae-93d41e293f76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================\n",
      "Running engine tests (plain asserts, no pytest)\n",
      "========================================================================\n",
      "[PASS] test_forward_ops\n",
      "[PASS] test_matvec_forward\n",
      "[PASS] test_backward_scalar_graph\n",
      "[PASS] test_backward_vector_graph\n",
      "[PASS] test_shared_node_simple\n",
      "[PASS] test_shared_node_deep\n",
      "[PASS] test_shared_node_multiple_backward\n",
      "[PASS] test_shared_node_vector\n",
      "[PASS] test_requires_grad_false_constant\n",
      "[PASS] test_broadcast_add_backward\n",
      "[PASS] test_broadcast_mul_backward\n",
      "[PASS] test_shape_mismatch_errors\n",
      "[PASS] test_zero_grad_graph_clears_all\n",
      "[PASS] test_accumulation_semantics_leaves_accumulate_intermediates_reset\n",
      "[PASS] test_gradcheck_tanh_chain\n",
      "[PASS] test_gradcheck_matvec\n",
      "[PASS] test_parameter_and_optimizers\n",
      "========================================================================\n",
      "Summary: 17 passed, 0 failed, total 17\n",
      "========================================================================\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ordon\\anaconda3\\envs\\autodiff\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3707: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# engine_tests.py\n",
    "import math\n",
    "import sys\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Helpers\n",
    "# ---------------------------------------------------------------------\n",
    "ATOL = 1e-6\n",
    "RTOL = 1e-6\n",
    "FD_EPS = 1e-5\n",
    "\n",
    "\n",
    "def assert_close(a, b, atol=ATOL, rtol=RTOL, msg=\"\"):\n",
    "    if not math.isclose(float(a), float(b), abs_tol=atol, rel_tol=rtol):\n",
    "        raise AssertionError(msg or f\"Expected {b}, got {a}\")\n",
    "\n",
    "\n",
    "def assert_close_list(a, b, atol=ATOL, rtol=RTOL, msg=\"\"):\n",
    "    if len(a) != len(b):\n",
    "        raise AssertionError(msg or f\"Length mismatch: {len(a)} vs {len(b)}\")\n",
    "    for i, (ai, bi) in enumerate(zip(a, b)):\n",
    "        if not math.isclose(float(ai), float(bi), abs_tol=atol, rel_tol=rtol):\n",
    "            raise AssertionError(msg or f\"Mismatch at idx={i}: got {ai}, expected {bi}\")\n",
    "\n",
    "\n",
    "def expect_raises(exc_type, fn, *args, **kwargs):\n",
    "    try:\n",
    "        fn(*args, **kwargs)\n",
    "    except exc_type:\n",
    "        return\n",
    "    except Exception as e:\n",
    "        raise AssertionError(f\"Expected {exc_type.__name__}, got {type(e).__name__}: {e}\") from e\n",
    "    raise AssertionError(f\"Expected {exc_type.__name__} but no exception was raised\")\n",
    "\n",
    "\n",
    "def scalar_objective_from_fn(fn, x_vals):\n",
    "    \"\"\"\n",
    "    Build a graph for L(x) and return (x_node, loss_node).\n",
    "    If fn returns a vector, we reduce with sum_pop to scalar.\n",
    "    \"\"\"\n",
    "    x = PopulationNode(list(x_vals))\n",
    "    out = fn(x)\n",
    "    if len(out.data) != 1:\n",
    "        out = sum_pop(out)\n",
    "    return x, out\n",
    "\n",
    "\n",
    "def finite_diff_grad(fn, x0, eps=FD_EPS):\n",
    "    \"\"\"\n",
    "    Central difference gradient for scalar objective L(x).\n",
    "    \"\"\"\n",
    "    g = [0.0] * len(x0)\n",
    "    for i in range(len(x0)):\n",
    "        xp = list(x0)\n",
    "        xm = list(x0)\n",
    "        xp[i] += eps\n",
    "        xm[i] -= eps\n",
    "\n",
    "        _, out_p = scalar_objective_from_fn(fn, xp)\n",
    "        _, out_m = scalar_objective_from_fn(fn, xm)\n",
    "\n",
    "        Lp = out_p.data[0]\n",
    "        Lm = out_m.data[0]\n",
    "        g[i] = (Lp - Lm) / (2 * eps)\n",
    "    return g\n",
    "\n",
    "\n",
    "def engine_grad(fn, x0):\n",
    "    x, out = scalar_objective_from_fn(fn, x0)\n",
    "    out.backprop()\n",
    "    return list(x.grad), out\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Shared‑node correctness (multi‑path gradient flow)\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def test_shared_node_simple():\n",
    "    \"\"\"\n",
    "    z = m + m where m = x * y\n",
    "    Ensures correct accumulation on shared subgraphs.\n",
    "    \"\"\"\n",
    "    x = PopulationNode([2.0], requires_grad=True)\n",
    "    y = PopulationNode([3.0], requires_grad=True)\n",
    "\n",
    "    m = x * y\n",
    "    z = m + m   # shared node\n",
    "\n",
    "    x.zero_grad()\n",
    "    y.zero_grad()\n",
    "    z.backprop()\n",
    "\n",
    "    # dz/dx = 2*y = 6\n",
    "    # dz/dy = 2*x = 4\n",
    "    assert x.grad == [6.0]\n",
    "    assert y.grad == [4.0]\n",
    "\n",
    "\n",
    "def test_shared_node_deep():\n",
    "    \"\"\"\n",
    "    z = m + (m * 2) where m = x * y\n",
    "    Ensures correct multi‑path gradient flow.\n",
    "    \"\"\"\n",
    "    x = PopulationNode([2.0], requires_grad=True)\n",
    "    y = PopulationNode([5.0], requires_grad=True)\n",
    "\n",
    "    m = x * y\n",
    "    z = m + (m * 2.0)   # 3*m total\n",
    "\n",
    "    x.zero_grad()\n",
    "    y.zero_grad()\n",
    "    z.backprop()\n",
    "\n",
    "    # dz/dx = 3*y = 15\n",
    "    # dz/dy = 3*x = 6\n",
    "    assert x.grad == [15.0]\n",
    "    assert y.grad == [6.0]\n",
    "\n",
    "\n",
    "def test_shared_node_multiple_backward():\n",
    "    \"\"\"\n",
    "    Ensures:\n",
    "    - leaf nodes accumulate across backward calls\n",
    "    - shared intermediate nodes DO NOT accumulate across calls\n",
    "    \"\"\"\n",
    "    x = PopulationNode([2.0], requires_grad=True)\n",
    "    y = PopulationNode([3.0], requires_grad=True)\n",
    "\n",
    "    m = x * y\n",
    "    z = m + m\n",
    "\n",
    "    x.zero_grad()\n",
    "    y.zero_grad()\n",
    "\n",
    "    # First backward\n",
    "    z.backprop()\n",
    "    g1x, g1y = x.grad[:], y.grad[:]\n",
    "\n",
    "    # Second backward (no zero_grad)\n",
    "    z.backprop()\n",
    "    g2x, g2y = x.grad[:], y.grad[:]\n",
    "\n",
    "    # Leaves accumulate\n",
    "    assert g2x == [2 * g1x[0]]\n",
    "    assert g2y == [2 * g1y[0]]\n",
    "\n",
    "    # Intermediate node m should NOT accumulate across calls\n",
    "    # (implicitly tested because if it did, leaf grads would be wrong)\n",
    "    assert g2x == [12.0]   # 6 → 12\n",
    "    assert g2y == [8.0]    # 4 → 8\n",
    "\n",
    "\n",
    "def test_shared_node_vector():\n",
    "    \"\"\"\n",
    "    Shared node with vector outputs.\n",
    "    v = [1,2,3]\n",
    "    m = v * 2\n",
    "    z = m + m\n",
    "    dz/dv = 4\n",
    "    \"\"\"\n",
    "    v = PopulationNode([1.0, 2.0, 3.0], requires_grad=True)\n",
    "\n",
    "    m = v * 2.0\n",
    "    z = m + m\n",
    "\n",
    "    v.zero_grad()\n",
    "    z.backprop()\n",
    "\n",
    "    assert v.grad == [4.0, 4.0, 4.0]\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Tests\n",
    "# ---------------------------------------------------------------------\n",
    "def test_forward_ops():\n",
    "    x = PopulationNode([1.0, 2.0, 3.0])\n",
    "    y = PopulationNode([10.0, 20.0, 30.0])\n",
    "\n",
    "    assert x.data == [1.0, 2.0, 3.0]\n",
    "    assert (x + y).data == [11.0, 22.0, 33.0]\n",
    "    assert (x * y).data == [10.0, 40.0, 90.0]\n",
    "    assert (y - x).data == [9.0, 18.0, 27.0]\n",
    "\n",
    "    t = x.tanh().data\n",
    "    assert all(-1.0 < v < 1.0 for v in t)\n",
    "\n",
    "    s = sum_pop(x)\n",
    "    assert s.data == [6.0]\n",
    "\n",
    "\n",
    "def test_matvec_forward():\n",
    "    A = [[1.0, 2.0], [3.0, 4.0]]\n",
    "    x = PopulationNode([5.0, 6.0])\n",
    "    y = matvec(A, x)\n",
    "    assert y.data == [17.0, 39.0]\n",
    "\n",
    "\n",
    "def test_backward_scalar_graph():\n",
    "    # z = x*y + x, x=2, y=3 => dz/dx = 4, dz/dy = 2\n",
    "    x = PopulationNode(2.0)\n",
    "    y = PopulationNode(3.0)\n",
    "    z = x * y + x\n",
    "    z.backprop()\n",
    "    assert_close_list(x.grad, [4.0])\n",
    "    assert_close_list(y.grad, [2.0])\n",
    "    assert_close_list(z.grad, [1.0])\n",
    "\n",
    "\n",
    "def test_backward_vector_graph():\n",
    "    # L = sum(x*y + x) for x=[1,2,3], y=[4,5,6]\n",
    "    # dL/dx = y + 1 = [5,6,7], dL/dy = x = [1,2,3]\n",
    "    x = PopulationNode([1.0, 2.0, 3.0])\n",
    "    y = PopulationNode([4.0, 5.0, 6.0])\n",
    "    L = sum_pop(x * y + x)\n",
    "    L.backprop()\n",
    "\n",
    "    assert_close_list(x.grad, [5.0, 6.0, 7.0])\n",
    "    assert_close_list(y.grad, [1.0, 2.0, 3.0])\n",
    "\n",
    "\n",
    "def test_requires_grad_false_constant():\n",
    "    x = PopulationNode([1.0, 2.0, 3.0])\n",
    "    c = PopulationNode([10.0, 10.0, 10.0], requires_grad=False)\n",
    "    L = sum_pop(x * c)\n",
    "    L.backprop()\n",
    "\n",
    "    assert_close_list(x.grad, [10.0, 10.0, 10.0])\n",
    "    assert_close_list(c.grad, [0.0, 0.0, 0.0])\n",
    "\n",
    "\n",
    "def test_broadcast_add_backward():\n",
    "    # out = sum(x + v) where x is scalar => dx = len(v), dv_i = 1\n",
    "    x = PopulationNode(2.0)\n",
    "    v = PopulationNode([1.0, 2.0, 3.0])\n",
    "    out = sum_pop(x + v)\n",
    "    out.backprop()\n",
    "\n",
    "    assert_close_list(x.grad, [3.0])\n",
    "    assert_close_list(v.grad, [1.0, 1.0, 1.0])\n",
    "\n",
    "\n",
    "def test_broadcast_mul_backward():\n",
    "    # out = sum(x * v) where x scalar => dx = sum(v), dv_i = x\n",
    "    x = PopulationNode(2.0)\n",
    "    v = PopulationNode([1.0, 2.0, 3.0])\n",
    "    out = sum_pop(x * v)\n",
    "    out.backprop()\n",
    "\n",
    "    assert_close_list(x.grad, [6.0])\n",
    "    assert_close_list(v.grad, [2.0, 2.0, 2.0])\n",
    "\n",
    "\n",
    "def test_shape_mismatch_errors():\n",
    "    a = PopulationNode([1.0, 2.0])\n",
    "    b = PopulationNode([1.0, 2.0, 3.0])\n",
    "    expect_raises(ValueError, lambda: a + b)\n",
    "    expect_raises(ValueError, lambda: a * b)\n",
    "\n",
    "    A = [[1.0, 2.0], [3.0, 4.0]]\n",
    "    x = PopulationNode([1.0, 2.0, 3.0])\n",
    "    expect_raises(ValueError, lambda: matvec(A, x))\n",
    "\n",
    "\n",
    "def test_zero_grad_graph_clears_all():\n",
    "    x = PopulationNode(2.0)\n",
    "    y = PopulationNode(3.0)\n",
    "    z = x * y + x\n",
    "    z.backprop()\n",
    "    assert x.grad != [0.0] and y.grad != [0.0] and z.grad != [0.0]\n",
    "\n",
    "    z.zero_grad_graph()\n",
    "    assert_close_list(x.grad, [0.0])\n",
    "    assert_close_list(y.grad, [0.0])\n",
    "    assert_close_list(z.grad, [0.0])\n",
    "\n",
    "\n",
    "def test_accumulation_semantics_leaves_accumulate_intermediates_reset():\n",
    "    \"\"\"\n",
    "    This matches your \"Approach A\":\n",
    "      - leaf grads accumulate across repeated backprop calls\n",
    "      - intermediate grads should NOT accumulate (they should be reset inside backprop)\n",
    "    \"\"\"\n",
    "    x = PopulationNode(2.0)\n",
    "    y = PopulationNode(3.0)\n",
    "    a = x * y          # intermediate\n",
    "    z = a + x          # output\n",
    "\n",
    "    z.backprop()\n",
    "    g1x, g1y, g1a = x.grad[0], y.grad[0], a.grad[0]\n",
    "\n",
    "    z.backprop()  # no manual reset\n",
    "    g2x, g2y, g2a = x.grad[0], y.grad[0], a.grad[0]\n",
    "\n",
    "    assert_close(g2x, 2 * g1x, msg=\"Leaf x.grad should accumulate (double).\")\n",
    "    assert_close(g2y, 2 * g1y, msg=\"Leaf y.grad should accumulate (double).\")\n",
    "    assert_close(g2a, g1a, msg=\"Intermediate a.grad should NOT accumulate under Approach A.\")\n",
    "\n",
    "\n",
    "def test_gradcheck_tanh_chain():\n",
    "    # L = sum(tanh(x) * x)\n",
    "    fn = lambda x: sum_pop(x.tanh() * x)\n",
    "    x0 = [0.1, -0.7, 1.3]\n",
    "    g_eng, _ = engine_grad(fn, x0)\n",
    "    g_fd = finite_diff_grad(fn, x0)\n",
    "    # tanh can be slightly sensitive; loosen tolerance a bit\n",
    "    assert_close_list(g_eng, g_fd, atol=2e-4, rtol=2e-4)\n",
    "\n",
    "\n",
    "def test_gradcheck_matvec():\n",
    "    A = [\n",
    "        [0.3, 1.7, -2.0],\n",
    "        [1.0, -0.5, 0.2],\n",
    "    ]\n",
    "    fn = lambda x: sum_pop(matvec(A, x))\n",
    "    x0 = [0.4, -0.8, 1.1]\n",
    "    g_eng, _ = engine_grad(fn, x0)\n",
    "    g_fd = finite_diff_grad(fn, x0)\n",
    "    assert_close_list(g_eng, g_fd, atol=2e-4, rtol=2e-4)\n",
    "\n",
    "\n",
    "def test_parameter_and_optimizers():\n",
    "    # Parameter.step\n",
    "    w = Parameter([1.0, -2.0, 0.5])\n",
    "    x = PopulationNode([3.0, 4.0, 5.0], requires_grad=False)\n",
    "    loss = sum_pop(w * x)\n",
    "    loss.backprop()\n",
    "\n",
    "    assert_close_list(w.grad, [3.0, 4.0, 5.0])\n",
    "    old = w.data[:]\n",
    "    lr = 0.1\n",
    "    w.step(lr)\n",
    "    assert_close_list(w.data, [old[i] - lr * w.grad[i] for i in range(3)])\n",
    "\n",
    "    # GD optimizer + zero_grad\n",
    "    w2 = Parameter([1.0, 2.0])\n",
    "    x2 = PopulationNode([10.0, -1.0], requires_grad=False)\n",
    "    loss2 = sum_pop(w2 * x2)\n",
    "    loss2.backprop()\n",
    "    opt = GD([w2], lr=0.5)\n",
    "    opt.step()\n",
    "    assert_close_list(w2.data, [1.0 - 0.5 * 10.0, 2.0 - 0.5 * (-1.0)])\n",
    "    opt.zero_grad()\n",
    "    assert_close_list(w2.grad, [0.0, 0.0])\n",
    "\n",
    "    # Momentum reference math (deterministic)\n",
    "    w3 = Parameter([1.0, 2.0])\n",
    "    w3.grad = [3.0, -4.0]\n",
    "    optm = Momentum([w3], lr=0.1, beta=0.9)\n",
    "\n",
    "    # step1: v=[-0.3, 0.4], w=[0.7,2.4]\n",
    "    optm.step()\n",
    "    assert_close_list(w3.data, [0.7, 2.4], atol=1e-9, rtol=0.0)\n",
    "\n",
    "    # step2: v=0.9*v - lr*grad => [-0.57, 0.76], w=[0.13,3.16]\n",
    "    w3.grad = [3.0, -4.0]\n",
    "    optm.step()\n",
    "    assert_close_list(w3.data, [0.13, 3.16], atol=1e-9, rtol=0.0)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Minimal runner\n",
    "# ---------------------------------------------------------------------\n",
    "TESTS = [\n",
    "    test_forward_ops,\n",
    "    test_matvec_forward,\n",
    "    test_backward_scalar_graph,\n",
    "    test_backward_vector_graph,\n",
    "    test_shared_node_simple,\n",
    "    test_shared_node_deep,\n",
    "    test_shared_node_multiple_backward,\n",
    "    test_shared_node_vector,\n",
    "    test_requires_grad_false_constant,\n",
    "    test_broadcast_add_backward,\n",
    "    test_broadcast_mul_backward,\n",
    "    test_shape_mismatch_errors,\n",
    "    test_zero_grad_graph_clears_all,\n",
    "    test_accumulation_semantics_leaves_accumulate_intermediates_reset,\n",
    "    test_gradcheck_tanh_chain,\n",
    "    test_gradcheck_matvec,\n",
    "    test_parameter_and_optimizers,\n",
    "]\n",
    "\n",
    "\n",
    "def run_all():\n",
    "    passed = 0\n",
    "    failed = 0\n",
    "\n",
    "    print(\"=\" * 72)\n",
    "    print(\"Running engine tests (plain asserts, no pytest)\")\n",
    "    print(\"=\" * 72)\n",
    "\n",
    "    for t in TESTS:\n",
    "        name = t.__name__\n",
    "        try:\n",
    "            t()\n",
    "            print(f\"[PASS] {name}\")\n",
    "            passed += 1\n",
    "        except Exception as e:\n",
    "            print(f\"[FAIL] {name}: {e}\")\n",
    "            traceback.print_exc()\n",
    "            failed += 1\n",
    "\n",
    "    print(\"=\" * 72)\n",
    "    print(f\"Summary: {passed} passed, {failed} failed, total {passed + failed}\")\n",
    "    print(\"=\" * 72)\n",
    "\n",
    "    return 0 if failed == 0 else 1\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sys.exit(run_all())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e73239-8fe0-4743-9548-350ee3c084f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2a1c6c-6fa0-4f88-b16b-208c776f8383",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e516341a-fdd7-4175-8bdb-8df788db9ffa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
