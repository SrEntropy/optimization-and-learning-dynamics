# Optimization and learning-dynamics
- Purpose: Prove mathematical maturity by understanding how learning works under the hood

## Motivation: 
- Can I derive learning rules?
- Can I reason about stability
- Can I connect discrete optimization to continuous dynamics?

- Mathematical Foundations
- Design decisions
- Key experiments
- What does this teach about learning systems?
- How does this connect to robotics and NeuroAI?  
 
## What it contains
- Backprop from scratch 
- Autograd engine (minimal)
- Gradient descent variants
- Visualization of loss landscapes
- Simple ODE-based neural dynamics

## Tech
- Numpy only
- No Pytorch here

## Core Components (implementation)
- A. Minimal Tensor + Autograd Engine
- B. Backpropagation From First Principles
- C. Optimization Algorithms
- D. Loss Landscapes & Geometry
- E. Continuous-Time Gradient Flow

# Experiments
- 1 XOR From Scratch
- 2 Vanishing/Exploding Gradients
- 3 Stability of Training
- 4 Gradient Flow vs GD














