{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6896968b-1807-4177-89a8-c429b4b3c554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"src\")\n",
    "\n",
    "from core.populationNode import PopulationNode\n",
    "from core.ops import _as_node, _broadcast_to_match, add, sub, mul, sum_pop, matvec\n",
    "from core.parameter import Parameter\n",
    "from models.activations import tanh\n",
    "from core.optim import GD, Momentum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96ef3ace-031d-4d4c-9b2b-17b418136814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PopulationNode( data=[2.0, 3.0], grad=[0.0, 0.0], op='leaf')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = PopulationNode([2,3])\n",
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3124d8d5-a37f-4f90-ac52-050a5c32c4e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PopulationNode( data=[4.0, 4.0], grad=[0.0, 0.0], op='leaf')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = PopulationNode([4,4])\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4857f1d5-928d-4422-a18d-fafb08fdd543",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = 3\n",
    "w2 = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e829cd6e-ef49-4375-8c47-97e6c62dc376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PopulationNode( data=[4.0, 9.0], grad=[0.0, 0.0], op='*')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = x1*x1\n",
    "l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29beb4b1-1d36-47d1-a27d-0b444df9403e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PopulationNode( data=[8.0, 8.0], grad=[0.0, 0.0], op='*')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2 = x2 * w2\n",
    "l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6c62c14-236e-4559-9723-c4871fa5494a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PopulationNode( data=[8.0, 12.0], grad=[0.0, 0.0], op='*')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 *x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc171dcd-d092-4876-bccc-3ff9548cdf8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9881a868-caaf-48ad-add6-c418a975dfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NODE] op=tanh, value=[0.4134608463957053], grad=[1.0] | <-- Parents=[[0.43977852653283767]]\n",
      "[NODE] op=tanh, value=[0.43977852653283767], grad=[0.829050128497747] | <-- Parents=[[0.4719561929220392]]\n",
      "[NODE] op=tanh, value=[0.4719561929220392], grad=[0.6687075620489031] | <-- Parents=[[0.5125841451937511]]\n",
      "[NODE] op=tanh, value=[0.5125841451937511], grad=[0.519757868915453] | <-- Parents=[[0.5662285757646344]]\n",
      "[NODE] op=tanh, value=[0.5662285757646344], grad=[0.3831953839732797] | <-- Parents=[[0.6419540521719098]]\n",
      "[NODE] op=tanh, value=[0.6419540521719098], grad=[0.26033727257499406] | <-- Parents=[[0.7614904913621627]]\n",
      "[NODE] op=tanh, value=[0.7614904913621627], grad=[0.15305097953277294] | <-- Parents=[[0.9997532108480275]]\n",
      "[NODE] op=tanh, value=[0.9997532108480275], grad=[0.06430164957431488] | <-- Parents=[[4.5]]\n",
      "[NODE] op=leaf, value=[4.5], grad=[3.1733982853148134e-05] | <-- Parents=[]\n"
     ]
    }
   ],
   "source": [
    "x = PopulationNode([4.5])\n",
    "y1 = x.tanh()\n",
    "y2 = y1.tanh()\n",
    "y3 = y2.tanh()\n",
    "y4 = y3.tanh()\n",
    "y5 = y4.tanh()\n",
    "y6 = y5.tanh()\n",
    "y7 = y6.tanh()\n",
    "y8 = y7.tanh()\n",
    "y8.backprop(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74335c2f-e8e1-4b7e-8892-5ad3a8850ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  PopulationNode( data=[-6.0, -3.0, -0.5, 0.0, 0.5, 3.0, 6.0], grad=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], op='leaf')\n",
      "[NODE] op=sum, value=[0.0], grad=[1.0] | <-- Parents=[[-0.9999877116507956, -0.9950547536867305, -0.46211715726000974, 0.0, 0.46211715726000974, 0.9950547536867305, 0.9999877116507956]]\n",
      "[NODE] op=tanh, value=[-0.9999877116507956, -0.9950547536867305, -0.46211715726000974, 0.0, 0.46211715726000974, 0.9950547536867305, 0.9999877116507956], grad=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0] | <-- Parents=[[-6.0, -3.0, -0.5, 0.0, 0.5, 3.0, 6.0]]\n",
      "[NODE] op=leaf, value=[-6.0, -3.0, -0.5, 0.0, 0.5, 3.0, 6.0], grad=[2.4576547405286142e-05, 0.009866037165440211, 0.7864477329659274, 1.0, 0.7864477329659274, 0.009866037165440211, 2.4576547405286142e-05] | <-- Parents=[]\n"
     ]
    }
   ],
   "source": [
    "xs = [-6, -3.0, -0.5, 0, .5, 3.0, 6]\n",
    "x = PopulationNode(xs)\n",
    "print(\"x: \", x)\n",
    "y = x.tanh()\n",
    "\n",
    "sum_pop(y).backprop(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f5e91af-f291-480c-a63c-0a3fdb42da53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum of unit gradient 0.7864477329659274\n"
     ]
    }
   ],
   "source": [
    "x_scalar = PopulationNode(0.5)\n",
    "y_scalar = x_scalar.tanh()\n",
    "y_scalar.backprop()\n",
    "print(\"sum of unit gradient\", sum(x_scalar.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13754c51-e757-4b6c-ac78-a9ad08a710b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3 [0.009866037165440211]\n",
      "-1 [0.41997434161402614]\n",
      "0 [1.0]\n",
      "1 [0.41997434161402614]\n",
      "3 [0.009866037165440211]\n"
     ]
    }
   ],
   "source": [
    "for v in [-3, -1, 0, 1, 3]:\n",
    "    x = PopulationNode(v)\n",
    "    y = x.tanh()\n",
    "    y.backprop()\n",
    "    print(v, x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c2c4504-f0fa-450a-993f-5b0f0dea8efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4.4, 4.4, 4.4],\n",
       " [3.9200000000000004, 3.9200000000000004, 3.9200000000000004],\n",
       " [3.5360000000000005, 3.5360000000000005, 3.5360000000000005],\n",
       " [3.2288000000000006, 3.2288000000000006, 3.2288000000000006],\n",
       " [2.9830400000000004, 2.9830400000000004, 2.9830400000000004],\n",
       " [2.7864320000000005, 2.7864320000000005, 2.7864320000000005],\n",
       " [2.6291456, 2.6291456, 2.6291456],\n",
       " [2.50331648, 2.50331648, 2.50331648],\n",
       " [2.402653184, 2.402653184, 2.402653184],\n",
       " [2.3221225472, 2.3221225472, 2.3221225472],\n",
       " [2.25769803776, 2.25769803776, 2.25769803776],\n",
       " [2.206158430208, 2.206158430208, 2.206158430208],\n",
       " [2.1649267441664, 2.1649267441664, 2.1649267441664],\n",
       " [2.13194139533312, 2.13194139533312, 2.13194139533312],\n",
       " [2.105553116266496, 2.105553116266496, 2.105553116266496]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "w = Parameter([5.0, 5.0, 5.0])\n",
    "opt = GD([w], lr=0.1)\n",
    "\n",
    "def pop_loss(w):\n",
    "    return sum((wi - 2.0) ** 2 for wi in w.data)\n",
    "\n",
    "history = []\n",
    "\n",
    "for _ in range(15):\n",
    "    L = pop_loss(w)\n",
    "    for i in range(len(w.data)):\n",
    "        w.grad[i] = 2 * (w.data[i] - 2.0)\n",
    "    opt.step()\n",
    "    history.append(w.data.copy())\n",
    "    opt.zero_grad()\n",
    "\n",
    "history\n",
    "\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7d5e210-c552-4254-b9f6-c29821850d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4.4, 4.4799999999999995, 4.32],\n",
       " [3.9200000000000004, 3.9839999999999995, 3.8560000000000003],\n",
       " [3.5360000000000005, 3.5871999999999997, 3.4848000000000003],\n",
       " [3.2288000000000006, 3.2697599999999998, 3.1878400000000005],\n",
       " [2.9830400000000004, 3.015808, 2.9502720000000004],\n",
       " [2.7864320000000005, 2.8126463999999998, 2.7602176000000003],\n",
       " [2.6291456, 2.65011712, 2.6081740800000004],\n",
       " [2.50331648, 2.520093696, 2.486539264],\n",
       " [2.402653184, 2.4160749568, 2.3892314112],\n",
       " [2.3221225472, 2.33285996544, 2.31138512896],\n",
       " [2.25769803776, 2.266287972352, 2.249108103168],\n",
       " [2.206158430208, 2.2130303778816, 2.1992864825344],\n",
       " [2.1649267441664, 2.17042430230528, 2.15942918602752],\n",
       " [2.13194139533312, 2.136339441844224, 2.127543348822016],\n",
       " [2.105553116266496, 2.109071553475379, 2.1020346790576125]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "w = Parameter([5.0, 5.1, 4.9])\n",
    "opt = GD([w], lr=0.1)\n",
    "\n",
    "history_asym = []\n",
    "\n",
    "for _ in range(15):\n",
    "    for i in range(len(w.data)):\n",
    "        w.grad[i] = 2 * (w.data[i] - 2.0)\n",
    "    opt.step()\n",
    "    history_asym.append(w.data.copy())\n",
    "    opt.zero_grad()\n",
    "\n",
    "history_asym\n",
    "\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e20e634-8012-41da-9762-5d7e165aefb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def _almost_equal_list(a, b, tol=1e-6):\n",
    "    assert len(a) == len(b)\n",
    "    for x, y in zip(a, b):\n",
    "        assert abs(x - y) < tol\n",
    "\n",
    "\n",
    "def test_matvec_forward_backward_diagonal():\n",
    "    A = [[2.0, 0.0],\n",
    "         [0.0, 3.0]]\n",
    "\n",
    "    x = PopulationNode([1.0, 1.0], requires_grad=True)\n",
    "    y = matvec(A, x)          # [2,3]\n",
    "    loss = sum_pop(y)         # 5\n",
    "\n",
    "    loss.zero_grad_graph()\n",
    "    loss.backprop(debug=True)\n",
    "\n",
    "    assert y.data == [2.0, 3.0]\n",
    "    # dL/dx = A^T @ [1,1] = [2,3]\n",
    "    _almost_equal_list(x.grad, [2.0, 3.0])\n",
    "\n",
    "\n",
    "def test_matvec_forward_backward_random():\n",
    "    rng = np.random.default_rng(0)\n",
    "    A = rng.normal(size=(3, 2))\n",
    "    x0 = rng.normal(size=(2,))\n",
    "\n",
    "    x = PopulationNode(x0.tolist(), requires_grad=True)\n",
    "    y = matvec(A, x)\n",
    "    loss = sum_pop(y)\n",
    "\n",
    "    loss.zero_grad_graph()\n",
    "    loss.backprop()\n",
    "\n",
    "    # analytic: A^T @ ones(3)\n",
    "    expected = A.T @ np.ones(3)\n",
    "    got = np.array(x.grad)\n",
    "    assert np.max(np.abs(got - expected)) < 1e-6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b96d2710-9403-4c2f-9c9c-11a9e01134e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NODE] op=sum, value=[5.0], grad=[1.0] | <-- Parents=[[2.0, 3.0]]\n",
      "[NODE] op=matvec, value=[2.0, 3.0], grad=[1.0, 1.0] | <-- Parents=[[1.0, 1.0]]\n",
      "[NODE] op=leaf, value=[1.0, 1.0], grad=[2.0, 3.0] | <-- Parents=[]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_matvec_forward_backward_diagonal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2111678-733d-4360-94f1-82bfc2d0e6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_matvec_forward_backward_random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a534792c-43d4-426c-89f5-dadad95ece35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Momentum tests passed.\n"
     ]
    }
   ],
   "source": [
    "def close(a, b, tol=1e-9):\n",
    "    return all(abs(x - y) < tol for x, y in zip(a, b))\n",
    "\n",
    "\n",
    "def test_momentum_beta_zero_equals_gd_update():\n",
    "    \"\"\"\n",
    "    If beta = 0, then:\n",
    "        v <- -lr * grad\n",
    "        w <- w + v = w - lr * grad\n",
    "    So it should behave like plain GD for that step.\n",
    "    \"\"\"\n",
    "    w = Parameter([1.0, 2.0])\n",
    "    w.grad = [1.0, 1.0]\n",
    "\n",
    "    opt = Momentum([w], lr=0.1, beta=0.0)\n",
    "    opt.step()\n",
    "\n",
    "    assert close(w.data, [0.9, 1.9])\n",
    "\n",
    "\n",
    "def test_momentum_two_steps_matches_hand_computation():\n",
    "    \"\"\"\n",
    "    Hand-check 2 steps with constant gradient to verify velocity accumulation.\n",
    "\n",
    "    Let w0 = 1.0, grad = 1.0, lr = 0.1, beta = 0.9\n",
    "    v0 = 0\n",
    "\n",
    "    Step 1:\n",
    "      v1 = 0.9*v0 - 0.1*1 = -0.1\n",
    "      w1 = w0 + v1 = 0.9\n",
    "\n",
    "    Step 2:\n",
    "      v2 = 0.9*v1 - 0.1*1 = 0.9*(-0.1) - 0.1 = -0.19\n",
    "      w2 = w1 + v2 = 0.9 - 0.19 = 0.71\n",
    "    \"\"\"\n",
    "    w = Parameter([1.0])\n",
    "    opt = Momentum([w], lr=0.1, beta=0.9)\n",
    "\n",
    "    # Step 1\n",
    "    w.grad = [1.0]\n",
    "    opt.step()\n",
    "    assert close(w.data, [0.9])\n",
    "\n",
    "    # Step 2 (same grad)\n",
    "    w.grad = [1.0]\n",
    "    opt.step()\n",
    "    assert close(w.data, [0.71])\n",
    "\n",
    "\n",
    "def test_momentum_zero_grad_clears_param_grads_only():\n",
    "    \"\"\"\n",
    "    zero_grad should clear Parameter.grad.\n",
    "    It should NOT reset velocity unless you explicitly add such behavior.\n",
    "    \"\"\"\n",
    "    w = Parameter([1.0])\n",
    "    opt = Momentum([w], lr=0.1, beta=0.9)\n",
    "\n",
    "    w.grad = [1.0]\n",
    "    opt.step()  # now velocity is non-zero\n",
    "\n",
    "    w.grad = [5.0]\n",
    "    opt.zero_grad()\n",
    "    assert w.grad == [0.0]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run without pytest\n",
    "    test_momentum_beta_zero_equals_gd_update()\n",
    "    test_momentum_two_steps_matches_hand_computation()\n",
    "    test_momentum_zero_grad_clears_param_grads_only()\n",
    "    print(\"All Momentum tests passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94a61ced-4c53-4d83-a27b-e0e44c5075aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Test 1: Scalar Test (Simple Chain Rule)\n",
      "============================================================\n",
      "z =  (expected [8.0]): [8.0]\n",
      "x.grad (expected [4.0]): [4.0]\n",
      "y.grad (expected [2.0]): [2.0]\n",
      "✓ Passed: gradients match analytical derivatives\n",
      "\n",
      "============================================================\n",
      "Test 2: Vector Test (Vector Chain Rule)\n",
      "============================================================\n",
      "x.grad (expected [1.0, 1.0]): [1.0, 1.0]\n",
      "y.grad (expected [1.0, 1.0]): [1.0, 1.0]\n",
      "✓ Passed: vector gradients match analytical derivatives\n",
      "\n",
      "============================================================\n",
      "Test 3: (w * x) -> sum -> + b -> tanh\n",
      "============================================================\n",
      "a.data: [0.7615941559557649]\n",
      "w.grad: [0.41997434161402614, 0.0] (expected [0.41997434161402614, 0.0])\n",
      "x.grad: [0.41997434161402614, -0.41997434161402614] (expected [0.41997434161402614, -0.41997434161402614])\n",
      "b.grad: [0.41997434161402614] (expected [0.41997434161402614])\n",
      "✓ Passed: tanh derivative and neuron gradients are correct\n",
      "\n",
      "============================================================\n",
      "Test 4: Gradient accumulation (no zero_grad)\n",
      "============================================================\n",
      "First x.grad: [4.0], second x.grad: [11.0]\n",
      "First y.grad: [2.0], second y.grad: [6.0]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Gradients should accumulate for x",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 93\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFirst x.grad: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mg1x\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, second x.grad: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mg2x\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     91\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFirst y.grad: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mg1y\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, second y.grad: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mg2y\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m close(g2x, [\u001b[32m2\u001b[39m * g1x[\u001b[32m0\u001b[39m]]), \u001b[33m\"\u001b[39m\u001b[33mGradients should accumulate for x\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m close(g2y, [\u001b[32m2\u001b[39m * g1y[\u001b[32m0\u001b[39m]]), \u001b[33m\"\u001b[39m\u001b[33mGradients should accumulate for y\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✓ Passed: gradients accumulate as expected\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAssertionError\u001b[39m: Gradients should accumulate for x"
     ]
    }
   ],
   "source": [
    "\n",
    "def header(title):\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(title)\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "def close(a, b, tol=1e-9):\n",
    "    return all(abs(x - y) < tol for x, y in zip(a, b))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Test 1: Scalar test\n",
    "# ------------------------------------------------------------\n",
    "header(\"Test 1: Scalar Test (Simple Chain Rule)\")\n",
    "\n",
    "x = PopulationNode(2.0)\n",
    "y = PopulationNode(3.0)\n",
    "z = x * y + x\n",
    "print(f\"z =  (expected [8.0]): {z.data}\")\n",
    "z.backprop()\n",
    "\n",
    "print(f\"x.grad (expected [4.0]): {x.grad}\")\n",
    "print(f\"y.grad (expected [2.0]): {y.grad}\")\n",
    "\n",
    "assert close(x.grad, [4.0]), \"Incorrect gradient for x\"\n",
    "assert close(y.grad, [2.0]), \"Incorrect gradient for y\"\n",
    "\n",
    "print(\"✓ Passed: gradients match analytical derivatives\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Test 2: Vector test\n",
    "# ------------------------------------------------------------\n",
    "header(\"Test 2: Vector Test (Vector Chain Rule)\")\n",
    "\n",
    "x = PopulationNode([1.0, 2.0])\n",
    "y = PopulationNode([3.0, 4.0])\n",
    "z = sum_pop(x + y)\n",
    "z.backprop()\n",
    "\n",
    "print(f\"x.grad (expected [1.0, 1.0]): {x.grad}\")\n",
    "print(f\"y.grad (expected [1.0, 1.0]): {y.grad}\")\n",
    "\n",
    "assert close(x.grad, [1.0, 1.0]), \"Incorrect gradient for x\"\n",
    "assert close(y.grad, [1.0, 1.0]), \"Incorrect gradient for y\"\n",
    "\n",
    "print(\"✓ Passed: vector gradients match analytical derivatives\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Test 3: Elementwise multiply + sum reduction + tanh neuron\n",
    "# ------------------------------------------------------------\n",
    "header(\"Test 3: (w * x) -> sum -> + b -> tanh\")\n",
    "\n",
    "w = PopulationNode([1.0, -1.0])\n",
    "x = PopulationNode([1.0, 0.0])\n",
    "b = PopulationNode(0.0)\n",
    "\n",
    "z = sum_pop(w * x) + b\n",
    "a = z.tanh()\n",
    "a.backprop()\n",
    "\n",
    "dadz = 1.0 - a.data[0] ** 2\n",
    "expected_w = [dadz * x.data[0], dadz * x.data[1]]\n",
    "expected_x = [dadz * w.data[0], dadz * w.data[1]]\n",
    "expected_b = [dadz]\n",
    "\n",
    "print(f\"a.data: {a.data}\")\n",
    "print(f\"w.grad: {w.grad} (expected {expected_w})\")\n",
    "print(f\"x.grad: {x.grad} (expected {expected_x})\")\n",
    "print(f\"b.grad: {b.grad} (expected {expected_b})\")\n",
    "\n",
    "assert close(w.grad, expected_w), \"Incorrect gradient for weights\"\n",
    "assert close(x.grad, expected_x), \"Incorrect gradient for inputs\"\n",
    "assert close(b.grad, expected_b), \"Incorrect gradient for bias\"\n",
    "\n",
    "print(\"✓ Passed: tanh derivative and neuron gradients are correct\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Test 4: Gradient accumulation\n",
    "# ------------------------------------------------------------\n",
    "header(\"Test 4: Gradient accumulation (no zero_grad)\")\n",
    "\n",
    "x = PopulationNode(2.0)\n",
    "y = PopulationNode(3.0)\n",
    "z = x * y + x\n",
    "\n",
    "z.backprop()\n",
    "g1x, g1y = x.grad[:], y.grad[:]\n",
    "\n",
    "z.backprop()  # no reset\n",
    "g2x, g2y = x.grad[:], y.grad[:]\n",
    "\n",
    "print(f\"First x.grad: {g1x}, second x.grad: {g2x}\")\n",
    "print(f\"First y.grad: {g1y}, second y.grad: {g2y}\")\n",
    "\n",
    "assert close(g2x, [2 * g1x[0]]), \"Gradients should accumulate for x\"\n",
    "assert close(g2y, [2 * g1y[0]]), \"Gradients should accumulate for y\"\n",
    "\n",
    "print(\"✓ Passed: gradients accumulate as expected\")\n",
    "\n",
    "print(\"\\nAll tests passed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f01685d-c1e1-42e4-ac3b-19f510ffbe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "w1 = Parameter([1.0])\n",
    "w2 = Parameter([2.0])\n",
    "\n",
    "w1.grad = [1.0]\n",
    "w2.grad = [2.0]\n",
    "\n",
    "opt = GD([w1, w2], lr=0.1)\n",
    "opt.step()\n",
    "\n",
    "assert w1.data == [0.9]\n",
    "assert w2.data == [1.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bd408c-d907-4ff1-bc4a-28e9d6dc8771",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.zero_grad()\n",
    "assert w1.grad == [0.0]\n",
    "assert w2.grad == [0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c42b23c-d8bf-4ada-92d5-6d2ebd9f6d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test_grad_accumulates_then_resets():\n",
    "    x = PopulationNode([1.0, 2.0], requires_grad=True)\n",
    "    print(x)\n",
    "    loss = sum_pop(x)  # loss = x1 + x2\n",
    "    print(x)\n",
    "    print(\"loss: \",loss)\n",
    "    loss.zero_grad_graph()\n",
    "    print(\"loss: \",loss)\n",
    "    loss.backprop(debug=True)\n",
    "    print(\"loss: \",loss)\n",
    "    g1 = list(x.grad)\n",
    "    print(g1)\n",
    "    assert g1 == [1.0, 1.0]\n",
    "    print(\"✓ Passed: gradients reset as expected\")\n",
    "\n",
    "    # backprop again without clearing => accumulates\n",
    "    loss.backprop(debug=True)\n",
    "    g2 = list(x.grad)\n",
    "    assert g2 == [2.0, 2.0]\n",
    "    print(\"✓ Passed: gradients accumulate as expected\")\n",
    "\n",
    "    # clear entire graph\n",
    "    loss.zero_grad_graph()\n",
    "    assert x.grad == [0.0, 0.0]\n",
    "    print(\"✓ Passed: clear entire graph's gradient as expected\")\n",
    "\n",
    "    print(\"\\nAll tests passed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909f916b-04c3-462e-a1c8-a7f86a6212d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_grad_accumulates_then_resets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd0b722-74e1-42a1-a653-5c17be2bbc58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
